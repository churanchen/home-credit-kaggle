{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!head -n 10 data/installments_payments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filename = 'data/application_train.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df.head(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['SK_ID_CURR', 'TARGET']].groupby('TARGET').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['SK_ID_CURR', 'TARGET']].groupby('TARGET').count()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/HomeCredit_columns_description.csv'\n",
    "desc_df = pd.read_csv(filename, encoding = \"ISO-8859-1\")\n",
    "desc_df[desc_df['Table'] == 'application_{train|test}.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_dist(df):\n",
    "    rows = df.shape[0]\n",
    "    target_dist_df = df[['SK_ID_CURR', 'TARGET']].groupby('TARGET').count()\n",
    "    target_dist_df['PERCENT'] = target_dist_df['SK_ID_CURR']*100/rows\n",
    "    return target_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/application_train.csv'\n",
    "train_df = pd.read_csv(filename)\n",
    "rows = train_df.shape[0]\n",
    "print(f'total rows: {rows}')\n",
    "print(get_target_dist(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(filename, nrows=10000)\n",
    "rows = train_df.shape[0]\n",
    "print(f'total rows: {rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_target_dist(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(filename)\n",
    "train_df = train_df.sample(n=10000)\n",
    "rows = train_df.shape[0]\n",
    "print(f'total rows: {rows}') \n",
    "print(get_target_dist(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols = train_df.columns[train_df.isnull().any()]\n",
    "nan_cnt = train_df[nan_cols].isnull().sum()\n",
    "print(nan_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = []\n",
    "data = []\n",
    "\n",
    "target_col = 'TARGET'\n",
    "features = list([x for x in train_df.columns if x != target_col])\n",
    "\n",
    "for row in train_df.to_dict('records'):\n",
    "    y.append(row[target_col])\n",
    "    data.append({k: row[k] for k in features})\n",
    "    \n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val, y_train, y_val = train_test_split(data, y, train_size=0.8, stratify=y)\n",
    "print(f'data_train cnt: {len(data_train)}')\n",
    "print(f'data_val cnt: {len(data_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_y_dist(y):\n",
    "    target2cnt = defaultdict(int)\n",
    "    for yi in y:\n",
    "        target2cnt[yi] += 1\n",
    "    \n",
    "    print('target\\tcnt\\tratio')\n",
    "    for target in sorted(target2cnt):\n",
    "        cnt = target2cnt[target]\n",
    "        print(f'{target}\\t{cnt}\\t{cnt/len(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('target distribution in training data')\n",
    "get_y_dist(y_train)\n",
    "\n",
    "print('\\ntarget distribution in validation data')\n",
    "get_y_dist(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "X_train = vectorizer.fit_transform(data_train)\n",
    "print(f'after vectorization: {X_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, feature in enumerate(vectorizer.feature_names_):\n",
    "    print(f'{i}\\t{feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "imputer = Imputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_train = scaler.fit_transform(X_train.toarray())\n",
    "print(f'X_train data type: {type(X_train)}')\n",
    "print(f'X_train: {X_train.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = vectorizer.transform(data_val)\n",
    "X_val = imputer.transform(X_val)\n",
    "X_val = scaler.transform(X_val)\n",
    "print(f'X_val data type: {type(X_val)}')\n",
    "print(f'X_val: {X_val.shape})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "start = time.time()\n",
    "print(f'Fitting model on {X_train.shape[0]} samples...')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "print('Finished model training in %.3f seconds.' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_weights(y):\n",
    "    weights = []\n",
    "    for yi in y:\n",
    "        weights.append(10 if yi else 1)\n",
    "    return np.array(weights)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "start = time.time()\n",
    "print(f'Fitting model on {X_train.shape[0]} samples...')\n",
    "model.fit(X_train, y_train, sample_weight=get_sample_weights(y_train))\n",
    "\n",
    "end = time.time()\n",
    "print('Finished model training in %.3f seconds.' % (end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, y_pred in enumerate(y_preds):\n",
    "    y_true = y_val[i]\n",
    "    print(f'i\\ty_pred: {y_pred}, y_true: {y_true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_val, y_pred=y_preds, labels=[0, 1], target_names=['NO', 'YES']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(X_val, y_val):\n",
    "    from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "    pos_idx = list(model.classes_).index(1)\n",
    "\n",
    "    print(X_val.shape, model.predict_proba(X_val).shape, pos_idx)\n",
    "    y_score = model.predict_proba(X_val)[:,pos_idx]\n",
    "    print(y_score.shape)\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_score, pos_label=1)\n",
    "    roc_auc = roc_auc_score(y_val, y_score)\n",
    "    \n",
    "    return roc_auc, fpr, tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "roc_auc, fpr, tpr = evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app_filename = 'data/previous_application.csv'\n",
    "prev_app_df = pd.read_csv(prev_app_filename)\n",
    "prev_app_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/HomeCredit_columns_description.csv'\n",
    "desc_df = pd.read_csv(filename, encoding = \"ISO-8859-1\")\n",
    "desc_df[desc_df['Table'] == 'previous_application.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_agg = prev_app_df.groupby('SK_ID_CURR')\n",
    "prev_df = prev_agg.agg({'SK_ID_PREV': 'count', 'AMT_ANNUITY': 'sum'}).rename(columns={\n",
    "    'SK_ID_PREV': 'PREV_APPS', 'AMT_ANNUITY': 'PREV_AMT_ANNUITY'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_prev_df = train_df.fillna(value=train_df.mean()).join(prev_df, on='SK_ID_CURR', how='left')\n",
    "curr_prev_df[['PREV_APPS', 'PREV_AMT_ANNUITY']] = curr_prev_df[['PREV_APPS', 'PREV_AMT_ANNUITY']].fillna(value=0)\n",
    "print(curr_prev_df.shape[0])\n",
    "curr_prev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/bureau.csv'\n",
    "bureau_df = pd.read_csv(filename)\n",
    "active_bureau_df = bureau_df[bureau_df['CREDIT_ACTIVE']=='Active']\n",
    "active_bureau_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_bureau_agg = active_bureau_df.groupby('SK_ID_CURR')\n",
    "active_bureau_agg_df = active_bureau_agg.agg({'AMT_CREDIT_SUM_DEBT': 'sum', 'CNT_CREDIT_PROLONG': 'sum'})\n",
    "active_bureau_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = curr_prev_df.join(active_bureau_agg_df, on='SK_ID_CURR', how='left')\n",
    "df[['AMT_CREDIT_SUM_DEBT', 'CNT_CREDIT_PROLONG']] = df[['AMT_CREDIT_SUM_DEBT', 'CNT_CREDIT_PROLONG']].fillna(value=0)\n",
    "print(df.shape[0])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(model, 'model')\n",
    "joblib.dump(scaler, 'scaler')\n",
    "joblib.dump(vectorizer, 'vectorizer')\n",
    "\n",
    "model = joblib.load('model')\n",
    "scaler = joblib.load('scaler')\n",
    "vectorizer = joblib.load('vectorizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
